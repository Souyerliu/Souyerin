---
title: 人工智能导论 Ch.3 机器学习（上）
date: 2025-10-07 22:40:43
categories: 
  - [study]
  - [计算机科学,人工智能导论]
tags:
  - 人工智能
  - 机器学习
  - 监督学习
cover: cover.jpg
---
# 机器学习基本概念
## 机器学习的核心：归纳偏置(Inductive Bias，或称为归纳性偏好)
  + 归纳——从一些例子（如训练数据）中寻找共性、泛化，形成一个较通用的规则的过程；
  + 偏置——机器学习算法在学习过程中对某种类型假设的 **偏好**，也可理解为学习过程中需要去预测 “其未遇到过的输入” 的结果时所进行的一些帮助它做出选择的 **假设**。
  + ![Inductive-Bias](InductiveBias.png)
## 机器学习的基本过程
  + 从原始数据中提取特征
  + 学习映射函数$f$
  + 通过映射函数$f$将原始数据映射到语义任务空间（属于希尔伯特空间），即寻找数据和任务目标之间的关系
## 机器学习典型例子
  + 基于图像数据进行类别分类
  + 基于文本数据进行情感分类
## 机器学习的分类
+ **监督学习(supervised learning)** ：数据有标签、一般为回归或分类等任务
  + 包括：决策树、朴素贝叶斯、SVM、KNN、Adaboost等
+ **非监督学习(un-supervised learning)** ：数据无标签、一般为聚类或若干降维任务
  + 包括：K-means，最大期望算法等，其中一个子分类是对比学习(Contrastive Learning)
+ 半监督学习(semi-supervised learning)：介于监督学习与半监督学习之间，部分数据有标签
  + 更接近现实情况（大量数据无法逐个标注）
+ **强化学习(reinforcement learning)** ：序列数据决策学习，一般为与从环境交互中学习
  + 如AlphaGo，无人驾驶等
## 机器学习：分类问题
+ 首先，提供具有标签的数据
+ 其次，映射函数$f$学习后得到隐含的模式，并进行挖掘和甄别
+ 最后，将具有特定特征的数据划分为特定类别
## 监督学习
### 监督学习的重要元素
1. 标注数据：标识了类别信息的数据（学什么）
2. 学习模型：如何学习得到映射模型（怎么学）
3. 损失函数：如何对学习结果进行度量（是否学到）
     + 是训练过程中主要改动的地方
### 损失函数
+ 在监督学习中，标注数据被划分为训练集，验证集和测试集
  + 训练集：用于训练学习模型
  + 验证集：在训练过程中验证模型的效果
  + 测试集：在训练完成后用于测试模型效果
+ 训练集中一共有$n$个标注数据，第$i$个标注数据记为$(x_i, y_i)$ ,其中第$i$个样本数据为$x_i$，$y_i$是$x_i$的标注信息。
+ 从训练数据中学习得到的映射函数记为$f$， $f$对$x_i$的预测结果记为$f(x_i)$ 。损失函数就是用来计算$x_i$真实值$y_i$与预测值$f(x_i)$之间差值的函数。
+ 很显然，在训练过程中希望映射函数在训练数据集上得到“损失”之和最小，即$\min \sum_{i=1}^n Loss(f(x_i),y_i)$
+ 常用损失函数：
  + 0-1损失函数：$Loss(y_i, f(x_i)) = \begin{cases}
                1, & f(x_i) \neq y_i \\
                0, & f(x_i) = y_i
                \end{cases}$
  + 平方损失函数：$Loss(y_i, f(x_i)) = (y_i - f(x_i))^2$
  + 绝对损失函数：$Loss(y_i, f(x_i)) = |y_i - f(x_i)|$
  + 对数（似然）损失函数：$Loss(y_i, P(y_i|x_i)) = -logP(y_i|x_i)$
### 经验风险与期望风险
+ **经验风险(empirical risk)** ：训练集中数据产生的损失。经验风险越小说明学习模型对训练数据拟合程度越好。
+ **期望风险(expected risk)** ：当测试集中存在无穷多数据时产生的损失。期望风险越小，学习所得模型越好。
+ 映射函数训练目标：
  1.  **经验风险最小化(empirical risk minimization)** ：$\min_{f\in\Phi}\frac{1}{n}\sum_{i=1}^nLoss(y_i,f(x_i))$ （即$n$个训练样本平均损失最小）
      + 这里的$Loss()$函数一般使用 **交叉熵损失函数(cross entropy loss)**。
  2.  **期望风险最小化(expected risk minimization)** ：$\min_{f\in\Phi}\int_{x\times y}Loss(y,f(x))P(x,y)dxdy$
      + $P(x,y)$是数据的真实联合概率分布，描述了数据生成过程。
      + 积分表示对所有可能的$(x,y)$求期望，即模型在未知数据上的平均表现。
+ 期望风险是模型关于联合分布期望损失，经验风险是模型关于训练样本集平均损失。
+ 根据大数定律，当样本容量趋于无穷时，经验风险趋于期望风险。所以在实践中很自然用经验风险来估计期望风险。
+ 由于现实中训练样本数目有限，用经验风险估计期望风险并不理想（$P(x,y)$不容易计算），要对经验风险进行一定的约束。
### 过拟合(over-fitting)与欠拟合(under-fitting)
+ 模型泛化能力与经验风险、期望风险的关系：
| 经验风险 | 期望风险 | 模型泛化能力 |
|:---:|:---:|:---:|
| 小（训练集上表现好） | 小（测试集上表现好） | 泛化能力强 |
| 小（训练集上表现好） | 大（测试集上表现不好） | 过学习（模型过于复杂） |
| 大（训练集上表现不好） | 大（测试集上表现不好） | 欠学习 |
| 大（训练集上表现不好） | 小（测试集上表现好） | “神仙算法” 或“黄梁美梦” |
+ 如何判断模型过拟合/欠拟合？
  + 在训练/测试过程中输出$Loss$函数值，综合判断。
+ 如何防止欠拟合？
  + 换性能更强的模型；
  + 给当前模型喂更多数据；
  + 调整损失函数。
+ 如何防止过拟合？
  + 见下文。
### 结构风险最小化(structural risk minimization)
+ 经验风险最小化：仅反映了局部数据
+ 期望风险最小化：无法得到全量数据
+ 结构风险最小化：
  + 为了防止过拟合，在经验风险上加上表示模型复杂度的**正则化项(regulatizer)** 或**惩罚项(penalty term)** ：
    + $\min_{f\in\Phi}\frac{1}{n}\sum_{i=1}^nLoss(y_i,f(x_i)) + \lambda J(f)$
    + 其中$Loss(y_i,f(x_i))$为经验风险，$\lambda J(f)$为模型复杂度
    + 这样可以在最小化经验风险与降低模型复杂度之间寻找平衡
  + 也可以使用$L1$范数正则化(LASSO正则化)：$\min_{f\in\Phi}\frac{1}{n}\sum_{i=1}^nLoss(y_i,f(x_i))+\lambda\|w\|_1$
    + 其中：
      + $|w|_1 = \sum_{j=1}^d |w_j|$ 是模型参数$w$的$L1$范数
      + $\lambda > 0$ 是正则化强度参数
      + $w$ 是模型的权重向量
    + 这样可以让数据稀疏化，以降低过拟合/欠拟合风险
### 监督学习的两种方法
+ 监督学习方法又可以分为 **生成方法(generative approach)** 和 **判别方法(discriminative approach)** 。所学到的模型分别称为 **生成模型(generative model)** 和 **判别模型(discriminative model)**.
1. 判别模型
   + 判别方法直接学习判别函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。
   + 判别模型关心在给定输入数据下，预测该数据的输出是什么。
   + 典型判别模型包括回归模型、神经网络、支持向量机和Ada boosting等。
2. 生成模型
   + 生成模型从数据中学习联合概率分布$P(X,Y)$（通过似然概率$P(Y|X)$和类概率$P(Y)$的乘积来求取）：
     + $P(Y|X)=\frac{P(X,Y)}{P(X)}$或者$P(Y|X)=\frac{P(X|Y)\times P(Y)}{P(X)}$ (注：似然概率$P(Y|X)$为计算导致样本$X$出现的模型参数值)
   + 典型方法为贝叶斯方法、隐马尔可夫链
   + 授之于鱼、不如授之于“渔”
   + 联合分布概率$P(X,Y)$或似然概率$P(X|Y)$求取很困难
# 回归分析
+ 分析不同变量之间存在关系的研究叫回归分析，刻画不同变量之间关系的模型被称为回归模型。
+ 一旦确定了回归模型，就可以进行预测等分析工作。
## 线性回归 (linear regression)
+ 回归（regression）一词的由来：
  + 源于英国生物学家兼统计学家高尔顿在研究父母身高与子女身高之间的关系时，发现父母平均身高每增加一个单位, 其成年子女平均身高只增加0.516个单位，它反映了这种“衰退(regression)”效应（“回归”到正常人平均身高）。
  + 虽然$x$和$y$之间并不总是具有“衰退”（回归）关系，但是“线性回归”这一名称就保留了下来了。
+ 线性回归的参数：需要从标注数据中学习得到(监督学习)
+ 注：以下回归参数推导过程不需要手推，会写代码套公式即可。
### 一元线性回归
+ 回归模型：$y=ax+b$
+ ![linear regression](linear_regression_example.png)
+ 求取：最佳回归模型是最小化残差平方和（Residual Sum of Squares，简称RSS）的均值，即要求$N$组$(x,y)$数据得到的残差平均值$\frac{1}{N}\Sigma(y-\widetilde{y})^2$最小。残差平均值最小只与参数$a$和$b$有关，最优解即是使得残差最小所对应的$a$和$b$的值。
+ 目标：寻找一组$a$和$b$，使得误差总和$L(a,b)=\sum_{i=1}^n(y_i-a\times x_i-b)^2$值最小。在线性回归中，解决如此目标的方法叫 **最小二乘法**。
+ 一般而言，要使函数具有最小值，可对$L(a,b)$的参数$a$和$b$分别求导，令其导数值为零，再求取参数$a$和$b$的取值。
  + 具体推导过程：
  $$\begin{gathered}
    \frac{\partial L(a,b)}{\partial a}=\sum_{i=1}^n2(y_i-ax_i-b)(-x_i)=0 
    \text{将}b=\bar{y}-a\bar{x}(\bar{y}=\frac{\sum_{i=1}^ny_i}{n},\bar{x}=\frac{\sum_{i=1}^nx_i}{n})\text{代入上式} 
    \to\sum_{i=1}^n(y_i-ax_i-\bar{y}+a\bar{x})(x_i)=0 
    \to\sum_{i=1}^n(y_ix_i-ax_ix_i-\bar{y}x_i+a\bar{x}x_i)=0 
    \to\sum_{i=1}^n(y_ix_i-\bar{y}x_i)-a\sum_{i=1}^n(x_ix_i-\bar{x}x_i)=0 
    \to(\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y})-a(\sum_{i=1}^nx_ix_i-n\bar{x}^2)=0 
    \to a=\frac{\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}^nx_ix_i-n\bar{x}^2}
    \end{gathered}
    \begin{aligned}
    & \frac{\partial L(a,b)}{\partial b}=\sum_{i=1}^n2(y_i-ax_i-b)(-1)=0 
    & \to\sum_{i=1}^n(y_i-ax_i-b)=0 
    & \to\sum_{i=1}^n(y_i)-a\sum_{i=1}^nx_i-\sum_{i=1}^nb=0 
    & \to n\overline{y}-an\overline{x}-nb=0 
    & \to b=\overline{y}-a\overline{x}
    \end{aligned}
  $$
+ 可以看出：只要给出了训练样本$(x_i, y_i) (i =1,\cdots,n)$,我们就可以从训练样本出发，建立一个线性回归方程，使得对训练样本数据而言，该线性回归方程预测的结果与样本标注结果之间的差值和最小。
### 多元线性回归
+ 多维数据特征中线性回归的问题定义如下：
  + 假设总共有$m$个训练数据$\{(x_i,y_i)\}_{i=1}^m$,其中$x_i=[x_{i,1},x_{i,2},\ldots,x_{i,D}]\in\mathbb{R}^D$,$D$为数据特征的维度,线性回归就是要找到一组参数$a=[a_0,a_1,...,a_D]$,使得线性函数：
  $$
  f(x_t)=a_0+\sum_{j=1}^Da_jx_{i,j}=a_0+a^Tx_i
  $$
+ 最小化均方误差函数(MSE): $J_m=\frac{1}{m}\sum_{i=1}^m(y_i-f(x_i))^2$
+ 为了方便,使用矩阵来表示所有的训练数据和数据标签：
$$
X=[x_1,...,x_m],\quad y=[y_1,...,y_m]
$$
+ 其中每一个数据$x_i$会扩展一个维度,其值为$1$,对应参数$a_0$。均方误差函数可以表示为：
$$
J_m(a)=\left(y-X^T\boldsymbol{a}\right)^T(\boldsymbol{y}-X^T\boldsymbol{a}) 
$$
+ 均方误差函数$J_n(\boldsymbol{a})$对所有参数$\boldsymbol{a}$求导可得：
$$
\nabla J(\boldsymbol{a})=-2X(y-X^T\boldsymbol{a}) 
$$
+ 因为均方误差函数$J_n(\boldsymbol{a})$是一个二次的凸函数,所以函数只存在一个极小值点,也同样是最小值点,所以令$\nabla J(a)=0$可得
$$
XX^T\boldsymbol{a}=X\boldsymbol{y} 
\boldsymbol{a}=\left(XX^T\right)^{-1}X\boldsymbol{y}
$$
### 逻辑斯蒂回归/对数几率回归
+ 线性回归一个明显的问题是对**离群点**（和大多数数据点距离较远的数据点，outlier）非常敏感，导致模型建模不稳定，使结果有偏，为了缓解这个问题（特别是在二分类场景中）带来的影响，可考虑**逻辑斯蒂回归(logistic regression)** 。
+ 逻辑斯蒂回归(logistic regression)就是在回归模型中引入 **sigmoid函数** 的一种非线性回归模型。Logistic回归模型可如下表示：
$$
y=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(w^Tx+b)}}\quad,\quad\text{其中}y\in(0,1),z=\boldsymbol{w}^T\boldsymbol{x}+\mathrm{b}
$$
+ 这里$\frac{1}{1+e^{-z}}$是sigmoid函数、$\boldsymbol{x}\in\mathbb{R}^d$是输入数据、$\boldsymbol{w}\in\mathbb{R}^d$和$b\in\mathbb{R}$是回归函数的参数。
+ ![sigmoid](Logistic-curve.svg)
+ Sigmoid函数的特点：
  + sigmoid函数是单调递增的，其值域为$(0,1)$，因此使sigmoid函数输出可作为概率值。在前面介绍的线性回归中，回归函数的值域一般为$(-\infty,+\infty)$
  + 对输入$z$范围没有限制，但当$z$大于一定数值后，函数输出无限趋近于$1$，而小于一定数值后，函数输出无限趋近于$0$。特别地，当$z=0$时，函数输出为$0.5$。这里$z$是输入数据$x$和回归函数参数$w$的内积结果（可视为$x$各维度进行加权叠加）
  + $x$各维度加权叠加之和结果取值在$0$附近时，函数输出值的变化幅度比较大（函数值变化陡峭），且是非线性变化。但是，各维度加权叠加之和结果取值很大或很小时，函数输出值几乎不变化，这是基于概率的一种认识与需要。
+ 逻辑斯蒂回归虽可用于对输入数据和输出结果之间复杂关系进行建模，但由于逻辑斯蒂回归函数的输出具有概率意义，使得逻辑斯蒂回归函数更多用于 **二分类问题**（$y = 1$表示输入数据$x$属于正例，$y = 0$表示输入数据$x$属于负例）\[对于多分类，可以使用softmax函数，原理类似，在此不赘述\]
+ $y=\frac{1}{1+e^{-(w^Tx+b)}}$可用来计算输入数据$x$属于正例的概率,这里$y$理解为输入数据$x$属于正例的概率、$1-y$理解为输入数据$x$为负例的概率，即$p(y = 1\mid x)$。
+ 我们现在对比值$\frac{p}{1-p}$取对数（即$\log\left(\frac{p}{1-p}\right)$）来表示输入数据$x$属于正例的概率。
  + $\frac{p}{1-p}$ 被称为 **几率(odds)** ,反映了输入数据$\boldsymbol{x}$作为正例的相对可能性。
  + $\frac{p}{1-p}$ 的 **对数几率(log odds)** 或logit函数可表示为$log\left(\frac{p}{1-p}\right)$。
+ 显然,可以得到$p(y=1|\boldsymbol{x})=h_\theta(x)=\frac{1}{1+e^{-(\boldsymbol{w}^T\boldsymbol{x}+b)}}$和$p(y=0|\boldsymbol{x})=1-h_\theta(x)=\frac{e^{-(\mathbf{w}^T\mathbf{x}+\mathbf{b})}}{1+e^{-(\mathbf{w}^T\mathbf{x}+\mathbf{b})}}$。$\theta$表示模型参数（$\theta=\{\mathbf{w},b\}$）。于是有：
$$
\mathrm{logit}\left(p(y=1|x)\right)=\log\left(\frac{p(y=1|x)}{p(y=0|x)}\right)=\log\left(\frac{p}{1-p}\right)=w^Tx+b
$$
+ 如果输入数据𝒙属于正例的概率大于其属于负例的概率，即$p(y = 1\mid x)>0.5$，则输入数据𝒙可被判断属于正例。这一结果等价于$\frac{p(y=1|x)}{p(y=0|x)}>1$，即
$\log\left(\frac{p(y=1|x)}{p(y=0|x)}\right)>\log1=0$，也就是$\mathbf{w}^T\mathbf{x}+\mathbf{b}>0$成立。
+ 从这里可以看出，logistic回归是一个线性模型。在预测时，可以计算线性函数$\mathbf{w}^T\mathbf{x}+\mathbf{b}$取值是否大于$0$来判断输入数据$x$的类别归属
# 决策树
+ 决策树是一种通过 **树形结构** 来进行分类的方法。在决策树中，树形结构中每个非叶子节点表示对分类目标在某个属性上的一个判断，每个分支代表基于该属性做出的一个判断，最后树形结构中每个叶子节点代表一种分类结果，所以决策树可以看作是一系列以叶子节点为输出的 **决策规则（Decision Rules）**
+ 例子：(虽然下面这图是个梗图，但也可以看作一种决策树)![chart](wd40.jpg)
+ 更一般地情况下，在构造决策树之前，会提供相应属性和决策的表格，通过机器学习训练得到决策树。
## 信息熵(entropy)
+ 由“信息论之父”香农提出
+ 设有$K$个信息，其组成了集合样本$D$，记第$k$个信息发生的概率为$p_k(1 \leq k \leq K)$”。如下定义这$K$个信息的信息熵：
$$
E(D)=-\sum_{k=1}^k p_k \log_2 p_k
$$
+ $E(D)$值越小，表示$D$包含的信息越确定，也称$D$的纯度越高。需要指出，所有$p_k$累加起来的和为$1$。